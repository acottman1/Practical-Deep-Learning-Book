{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "Copy of 1-feature-extraction.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9c62331134dd405ead1b123d6fec85b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c66950e91edb4a79912ca2b0417691f6",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d234b443c886405b88b31d0aa65bb428",
              "IPY_MODEL_22e6fe6af4d343a896dadaf1f6c0d29b"
            ]
          }
        },
        "c66950e91edb4a79912ca2b0417691f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d234b443c886405b88b31d0aa65bb428": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6bf67e97ebcd45eba1a44a46be5c8dcb",
            "_dom_classes": [],
            "description": " 17%",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 8663,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1503,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7e999ac888e740938e8144ee4ce13bad"
          }
        },
        "22e6fe6af4d343a896dadaf1f6c0d29b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d205070d7e8f48839d796336e0b54ae2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1503/8663 [02:36&lt;39:33,  3.02it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f6975d2ad28a4d18978ce7ef061cd960"
          }
        },
        "6bf67e97ebcd45eba1a44a46be5c8dcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7e999ac888e740938e8144ee4ce13bad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d205070d7e8f48839d796336e0b54ae2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f6975d2ad28a4d18978ce7ef061cd960": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/acottman1/Practical-Deep-Learning-Book/blob/master/code/chapter-4/1-feature-extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coSS8EUDx2tP",
        "colab_type": "text"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"center\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/acottman1/Practical-Deep-Learning-Book/code/chapter-4/1-feature-extraction.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/acottman1/Practical-Deep-Learning-Book/code/chapter-4/1-feature-extraction.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>\n",
        "\n",
        "\n",
        "This code is part of [Chapter 4 - Building a Reverse Image Search Engine: Understanding Embeddings](https://learning.oreilly.com/library/view/practical-deep-learning/9781492034858/ch04.html).\n",
        "\n",
        "Note: In order to run this notebook on Google Colab you need to [follow these instructions](https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb#scrollTo=WzIRIt9d2huC) so that the local data such as the images are available in your Google Drive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCbm9nfkx2tT",
        "colab_type": "text"
      },
      "source": [
        "# Feature Extraction\n",
        "\n",
        "This notebook is the first among six of the follow along Jupyter Notebook for Chapter 4. We will extract features from pretrained models like VGG-16, VGG-19, ResNet-50, InceptionV3 and MobileNet and benchmark them using the Caltech101 dataset.\n",
        "\n",
        "## Dataset:\n",
        "\n",
        "In the `data` directory of the repo, download the Caltech101 dataset (or try it on your dataset). \n",
        "\n",
        "```\n",
        "$ curl http://www.vision.caltech.edu/Image_Datasets/Caltech101/101_ObjectCategories.tar.gz --output caltech101.tar.gz\n",
        "\n",
        "$ tar -xvzf caltech101.tar.gz\n",
        "\n",
        "$ mv 101_ObjectCategories datasets/caltech101\n",
        "```\n",
        "Note that there is a 102nd category called â€˜BACKGROUND_Googleâ€™ consisting of random images not contained in the first 101 categories, which needs to be deleted before we start experimenting. \n",
        "\n",
        "```\n",
        "$ rm -rf datasets/caltech101/BACKGROUND_Google\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AewLPQpKdfw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3-turqex2tU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir -p ../../datasets\n",
        "!curl http://www.vision.caltech.edu/Image_Datasets/Caltech101/101_ObjectCategories.tar.gz --output ../../datasets/caltech101.tar.gz\n",
        "!tar -xvzf ../../datasets/caltech101.tar.gz --directory ../../datasets\n",
        "!mv ../../datasets/101_ObjectCategories ../../datasets/caltech101\n",
        "!rm -rf ../../datasets/caltech101/BACKGROUND_Google"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "WsSAUPPax2ta",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "import pickle\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "import math\n",
        "import tensorflow\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.applications.vgg19 import VGG19\n",
        "from tensorflow.keras.applications.mobilenet import MobileNet\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Flatten, Dense, Dropout, GlobalAveragePooling2D\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3wOQ1fOLiYa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "c095e8e6-3e07-4d62-ed43-873116f80b9a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUIUsrwyMRLH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvgKoNqOMSeL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "755be6d9-fc4c-435f-b252-29a88d96534e"
      },
      "source": [
        "os.chdir('/content')\n",
        "os.getcwd()\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpE02Sdrx2tf",
        "colab_type": "text"
      },
      "source": [
        "We will define a helper function that allows us to choose any pretrained model with all the necessary details for our experiments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BkVoIj8x2tj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_picker(name):\n",
        "    if (name == 'vgg16'):\n",
        "        model = VGG16(weights='imagenet',\n",
        "                      include_top=False,\n",
        "                      input_shape=(224, 224, 3),\n",
        "                      pooling='max')\n",
        "    elif (name == 'vgg19'):\n",
        "        model = VGG19(weights='imagenet',\n",
        "                      include_top=False,\n",
        "                      input_shape=(224, 224, 3),\n",
        "                      pooling='max')\n",
        "    elif (name == 'mobilenet'):\n",
        "        model = MobileNet(weights='imagenet',\n",
        "                          include_top=False,\n",
        "                          input_shape=(224, 224, 3),\n",
        "                          pooling='max',\n",
        "                          depth_multiplier=1,\n",
        "                          alpha=1)\n",
        "    elif (name == 'inception'):\n",
        "        model = InceptionV3(weights='imagenet',\n",
        "                            include_top=False,\n",
        "                            input_shape=(224, 224, 3),\n",
        "                            pooling='max')\n",
        "    elif (name == 'resnet'):\n",
        "        model = ResNet50(weights='imagenet',\n",
        "                         include_top=False,\n",
        "                         input_shape=(224, 224, 3),\n",
        "                        pooling='max')\n",
        "    elif (name == 'xception'):\n",
        "        model = Xception(weights='imagenet',\n",
        "                         include_top=False,\n",
        "                         input_shape=(224, 224, 3),\n",
        "                         pooling='max')\n",
        "    else:\n",
        "        print(\"Specified model not available\")\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSxkW_R3x2tn",
        "colab_type": "text"
      },
      "source": [
        "Now, let's put our function to use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AT7l5xsFx2to",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "a91f1518-0291-487a-f889-4c857db3a747"
      },
      "source": [
        "model_architecture = 'resnet'\n",
        "model = model_picker(model_architecture)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94773248/94765736 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apw2z3nIx2ts",
        "colab_type": "text"
      },
      "source": [
        "Let's define a function to extract image features given an image and a model. We developed a similar function in Chapter-2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzCWaD4ax2tt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_features(img_path, model):\n",
        "    input_shape = (224, 224, 3)\n",
        "    img = image.load_img(img_path,\n",
        "                         target_size=(input_shape[0], input_shape[1]))\n",
        "    img_array = image.img_to_array(img)\n",
        "    expanded_img_array = np.expand_dims(img_array, axis=0)\n",
        "    preprocessed_img = preprocess_input(expanded_img_array)\n",
        "    features = model.predict(preprocessed_img)\n",
        "    flattened_features = features.flatten()\n",
        "    normalized_features = flattened_features / norm(flattened_features)\n",
        "    return normalized_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9r1RrR-x2ty",
        "colab_type": "text"
      },
      "source": [
        "Let's see the feature length the model generates. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dNqZnphx2tz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bae0fd66-5180-4317-bb37-5e7602d8a4a4"
      },
      "source": [
        "features = extract_features('sample_data/cat.jpg', model)\n",
        "print(len(features))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2048\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-S5E64wix2t3",
        "colab_type": "text"
      },
      "source": [
        "Now, we will see how much time it takes to extract features of one image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoowHicox2t4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ad09ddb7-68f8-4a2c-a9a4-5c1ec04f39d7"
      },
      "source": [
        "%timeit features = extract_features('sample_data/cat.jpg', model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10 loops, best of 3: 88.1 ms per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7nyXRPyx2t7",
        "colab_type": "text"
      },
      "source": [
        "The time taken to extract features is dependent on a few factors such as image size, computing power etc. A better benchmark would be running the network over an entire dataset. A simple change to the existing code will allow this.\n",
        "\n",
        "Let's make a handy function to recursively get all the image files under a root directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qrn3N0w4x2t9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "extensions = ['.jpg', '.JPG', '.jpeg', '.JPEG', '.png', '.PNG']\n",
        "\n",
        "def get_file_list(root_dir):\n",
        "    file_list = []\n",
        "    for root, directories, filenames in os.walk(root_dir):\n",
        "        for filename in filenames:\n",
        "            if any(ext in filename for ext in extensions):\n",
        "                file_list.append(os.path.join(root, filename))\n",
        "    return file_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqO6X13hx2uB",
        "colab_type": "text"
      },
      "source": [
        "Now, let's run the extraction over the entire dataset and time it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ox0Jfc4x2uC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461,
          "referenced_widgets": [
            "9c62331134dd405ead1b123d6fec85b3",
            "c66950e91edb4a79912ca2b0417691f6",
            "d234b443c886405b88b31d0aa65bb428",
            "22e6fe6af4d343a896dadaf1f6c0d29b",
            "6bf67e97ebcd45eba1a44a46be5c8dcb",
            "7e999ac888e740938e8144ee4ce13bad",
            "d205070d7e8f48839d796336e0b54ae2",
            "f6975d2ad28a4d18978ce7ef061cd960"
          ]
        },
        "outputId": "903c16c7-7225-4dc1-e288-514339954957"
      },
      "source": [
        "# path to the your datasets\n",
        "root_dir = '/content/drive/My Drive/deep_learning_data/caltech101/101_ObjectCategories'\n",
        "filenames = sorted(get_file_list(root_dir))\n",
        "\n",
        "feature_list = []\n",
        "for i in tqdm_notebook(range(len(filenames))):\n",
        "    feature_list.append(extract_features(filenames[i], model))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9c62331134dd405ead1b123d6fec85b3",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=8663.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-3773ec239582>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfeature_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mfeature_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-3e7e21823682>\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(img_path, model)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     img = image.load_img(img_path,\n\u001b[0;32m----> 4\u001b[0;31m                          target_size=(input_shape[0], input_shape[1]))\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mimg_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mexpanded_img_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/preprocessing/image.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    299\u001b[0m   \"\"\"\n\u001b[1;32m    300\u001b[0m   return image.load_img(path, grayscale=grayscale, color_mode=color_mode,\n\u001b[0;32m--> 301\u001b[0;31m                         target_size=target_size, interpolation=interpolation)\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/utils.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    112\u001b[0m                           'The use of `load_img` requires PIL.')\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcolor_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'grayscale'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0;31m# if image is not already an 8-bit, 16-bit or 32-bit grayscale image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiIZso-1x2uF",
        "colab_type": "text"
      },
      "source": [
        "Now let's try the same with the Keras Image Generator functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeZ0bIaYx2uG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "84b97765-4372-40d0-ba06-4694574d7b2d"
      },
      "source": [
        "batch_size = 64\n",
        "datagen = tensorflow.keras.preprocessing.image.ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "\n",
        "generator = datagen.flow_from_directory(root_dir,\n",
        "                                        target_size=(224, 224),\n",
        "                                        batch_size=batch_size,\n",
        "                                        class_mode=None,\n",
        "                                        shuffle=False)\n",
        "\n",
        "num_images = len(generator.filenames)\n",
        "num_epochs = int(math.ceil(num_images / batch_size))\n",
        "\n",
        "start_time = time.time()\n",
        "feature_list = []\n",
        "feature_list = model.predict(generator, num_epochs)\n",
        "end_time = time.time()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 8663 images belonging to 101 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4bsX7WFx2uK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "5e38b040-1627-45ca-ef95-d6b54e15b048"
      },
      "source": [
        "for i, features in enumerate(feature_list):\n",
        "    feature_list[i] = features / norm(features)\n",
        "\n",
        "feature_list = feature_list.reshape(num_images, -1)\n",
        "\n",
        "print(\"Num images   = \", len(generator.classes))\n",
        "print(\"Shape of feature_list = \", feature_list.shape)\n",
        "print(\"Time taken in sec = \", end_time - start_time)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num images   =  8663\n",
            "Shape of feature_list =  (8663, 2048)\n",
            "Time taken in sec =  1922.6999862194061\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E87l_QtNx2uO",
        "colab_type": "text"
      },
      "source": [
        "### GPU Utilization's effect on time taken by varying batch size \n",
        "\n",
        "\n",
        "GPUs are optimized to parallelize the feature generation process and hence will give better results when multiple images are passed instead of just one image.\n",
        "The opportunity to improve can be seen based on GPU Utilization. Low GPU Utilization indicates an opportunity to further improve the througput.\n",
        "\n",
        "\n",
        "GPU Utilization can be seen using the nvidia-smi command. To update it every half a second\n",
        "\n",
        "    watch -n .5 nvidia-smi\n",
        "    \n",
        "To pool the GPU utilization every second and dump into a file\n",
        "\n",
        "    nvidia-smi --query-gpu=utilization.gpu --format=csv,noheader,nounits -f gpu_utilization.csv -l 1\n",
        "    \n",
        "To calculate median GPU Utilization from the file generated\n",
        "\n",
        "    sort -n gpu_utilization.csv | datamash median 1\n",
        "\n",
        "|Model |Time second (sec) | batch_size | % GPU Utilization | Implementation|\n",
        "|-|-|-|\n",
        "|Resnet50 | 124  | 1  | 52 | extract_features    |\n",
        "|Resnet50 | 98   | 1  | 72 | ImageDataGenerator |\n",
        "|Resnet50 | 57   | 2  | 81 | ImageDataGenerator |\n",
        "|Resnet50 | 40   | 4  | 88 | ImageDataGenerator |\n",
        "|Resnet50 | 34   | 8  | 94 | ImageDataGenerator |\n",
        "|Resnet50 | 29   | 16 | 97 | ImageDataGenerator |\n",
        "|Resnet50 | 28   | 32 | 97 | ImageDataGenerator |\n",
        "|Resnet50 | 28   | 64 | 98 | ImageDataGenerator |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAjfdinZx2uO",
        "colab_type": "text"
      },
      "source": [
        "### Some benchmarks on different model architectures to see relative speeds\n",
        "\n",
        "Keeping batch size of 64, benchmarking the different models\n",
        "\n",
        "|Model |items/second |\n",
        "|-|-|-|\n",
        "| VGG19     | 31.06 |\n",
        "| VGG16     | 28.16 | \n",
        "| Resnet50  | 28.48 | \n",
        "| Inception | 20.07 |\n",
        "| Mobilenet | 13.45 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwNh7YNFx2uQ",
        "colab_type": "text"
      },
      "source": [
        "Let's save the features as intermediate files to use later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJm87qFQx2uQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filenames = [root_dir + '/' + s for s in generator.filenames]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhmhClngeUXB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "74d375a5-0383-4688-d903-7c6486b18ff2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "319201"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "code_folding": [],
        "id": "eZI3oaiex2uV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pickle.dump(generator.classes, open('/content/drive/My Drive/deep_learning_data/caltech101/generator-classes.pickle', \n",
        "                                    'wb'))\n",
        "pickle.dump(filenames, open('/content/drive/My Drive/deep_learning_data/caltech101/filenames-caltech101.pickle', 'wb'))\n",
        "pickle.dump(\n",
        "    feature_list,\n",
        "    open('/content/drive/My Drive/deep_learning_data/caltech101/features-caltech101-' + model_architecture + '.pickle', 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8V52i8Wux2ua",
        "colab_type": "text"
      },
      "source": [
        "Let's train a finetuned model as well and save the features for that as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sbqRNO1x2ua",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRAIN_SAMPLES = 8677\n",
        "NUM_CLASSES = 101\n",
        "IMG_WIDTH, IMG_HEIGHT = 224, 224"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ckw65Ibx2ug",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input,\n",
        "                                   rotation_range=20,\n",
        "                                   width_shift_range=0.2,\n",
        "                                   height_shift_range=0.2,\n",
        "                                   zoom_range=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykW9c40-x2uk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dee6acaf-4bc5-42dc-d91d-97cd494103db"
      },
      "source": [
        "train_generator = train_datagen.flow_from_directory(root_dir,\n",
        "                                                    target_size=(IMG_WIDTH,\n",
        "                                                                 IMG_HEIGHT),\n",
        "                                                    batch_size=batch_size,\n",
        "                                                    shuffle=True,\n",
        "                                                    seed=12345,\n",
        "                                                    class_mode='categorical')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 8663 images belonging to 101 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmY-55-Hx2uo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_maker():\n",
        "    base_model = ResNet50(include_top=False,\n",
        "                           input_shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
        "    for layer in base_model.layers[:]:\n",
        "        layer.trainable = False\n",
        "    input = Input(shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
        "    custom_model = base_model(input)\n",
        "    custom_model = GlobalAveragePooling2D()(custom_model)\n",
        "    custom_model = Dense(64, activation='relu')(custom_model)\n",
        "    custom_model = Dropout(0.5)(custom_model)\n",
        "    predictions = Dense(NUM_CLASSES, activation='softmax')(custom_model)\n",
        "    return Model(inputs=input, outputs=predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-s7Zh1kyx2ur",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "2b162bed-2b79-4620-a9b8-46ad7c73446e"
      },
      "source": [
        "model_finetuned = model_maker()\n",
        "model_finetuned.compile(loss='categorical_crossentropy',\n",
        "              optimizer=tensorflow.keras.optimizers.Adam(0.001),\n",
        "              metrics=['acc'])\n",
        "model_finetuned.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=math.ceil(float(TRAIN_SAMPLES) / batch_size),\n",
        "    epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "136/136 [==============================] - 108s 797ms/step - loss: 2.9923 - acc: 0.3445\n",
            "Epoch 2/10\n",
            "136/136 [==============================] - 108s 792ms/step - loss: 1.8371 - acc: 0.5514\n",
            "Epoch 3/10\n",
            "136/136 [==============================] - 108s 792ms/step - loss: 1.3909 - acc: 0.6381\n",
            "Epoch 4/10\n",
            "136/136 [==============================] - 108s 796ms/step - loss: 1.1536 - acc: 0.6897\n",
            "Epoch 5/10\n",
            "136/136 [==============================] - 108s 792ms/step - loss: 1.0234 - acc: 0.7164\n",
            "Epoch 6/10\n",
            "136/136 [==============================] - 109s 799ms/step - loss: 0.9573 - acc: 0.7324\n",
            "Epoch 7/10\n",
            "136/136 [==============================] - 108s 796ms/step - loss: 0.8777 - acc: 0.7503\n",
            "Epoch 8/10\n",
            "136/136 [==============================] - 108s 795ms/step - loss: 0.8412 - acc: 0.7523\n",
            "Epoch 9/10\n",
            "136/136 [==============================] - 109s 799ms/step - loss: 0.7698 - acc: 0.7717\n",
            "Epoch 10/10\n",
            "136/136 [==============================] - 108s 795ms/step - loss: 0.7514 - acc: 0.7801\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1645480390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00PER7jQx2uu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_finetuned.save('/content/drive/My Drive/deep_learning_data/caltech101/model-finetuned.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvW9YrPix2uy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "2fd1b109-4b8b-480f-c877-c0d4400a4fd4"
      },
      "source": [
        "start_time = time.time()\n",
        "feature_list_finetuned = []\n",
        "feature_list_finetuned = model_finetuned.predict_generator(generator, num_epochs)\n",
        "end_time = time.time()\n",
        "\n",
        "for i, features_finetuned in enumerate(feature_list_finetuned):\n",
        "    feature_list_finetuned[i] = features_finetuned / norm(features_finetuned)\n",
        "\n",
        "feature_list = feature_list_finetuned.reshape(num_images, -1)\n",
        "\n",
        "print(\"Num images   = \", len(generator.classes))\n",
        "print(\"Shape of feature_list = \", feature_list.shape)\n",
        "print(\"Time taken in sec = \", end_time - start_time)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num images   =  8663\n",
            "Shape of feature_list =  (8663, 101)\n",
            "Time taken in sec =  29.546703577041626\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFf5HS5nx2u2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pickle.dump(\n",
        "    feature_list,\n",
        "    open('/content/drive/My Drive/deep_learning_data/caltech101/features-caltech101-resnet-finetuned.pickle', 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}